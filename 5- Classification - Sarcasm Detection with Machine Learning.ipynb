{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e2854e1",
   "metadata": {},
   "source": [
    "### 5- Classification - Sarcasm Detection with Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68746436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212a2990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK Natural Language Toolkit\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import BlanklineTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob # yazinin hangi dile yazildigini anliyoruz cok onemli\n",
    "from langdetect import detect\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import STOPWORDS\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8614493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53ac0337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json('Sarcasm.json',lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0e03393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  \n",
       "3  boehner just wants wife to listen, not come up...             1  \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "851e1c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "909724dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['headline'] = df['headline'].str.lower()\n",
    "df['headline'] = df['headline'].str.replace('[^\\w\\s]','')\n",
    "df['headline'] = df['headline'].str.replace('\\d','')\n",
    "df['headline'] = df['headline'].str.replace('\\n','')\n",
    "df['headline'] = df['headline'].str.replace('\\r','')\n",
    "df['headline'] = df['headline'].str.replace('\\t','')\n",
    "df['headline'] = df['headline'].str.replace('rt','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "205bfa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pr = PorterStemmer()\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed_tokens = [pr.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7fb4b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['headline'] = df['headline'].apply(tokenize_and_stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7615ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(text):\n",
    "    words=TextBlob(text).words\n",
    "    return [pr.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ab163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a462b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d92930c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c4dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78f5ee79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_link    0\n",
       "headline        0\n",
       "is_sarcastic    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "913e2177",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"headline\", \"is_sarcastic\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84433b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"headline\"]\n",
    "y = df[\"is_sarcastic\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9327d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(df[\"headline\"])\n",
    "y = np.array(df[\"is_sarcastic\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a869ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Evim\n",
      "[nltk_data]     Preschool\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Evim\n",
      "[nltk_data]     Preschool\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Evim\n",
      "[nltk_data]     Preschool\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5adf9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english', analyzer=lemma)\n",
    "x = cv.fit_transform(x) # Fit the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7089e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "851db7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef6f1510",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=modelb.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a672ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8498689629352303"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelb.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53522ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rc=RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69cf9934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8229127667540247"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=rc.fit(x_train,y_train)\n",
    "rc.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01d9fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "576afb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model1,open('sarcasm_detection.h5','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15cc1f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a Text: every bird has potential to teach\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "user = input(\"Enter a Text: \")\n",
    "data = cv.transform([user]).toarray()\n",
    "output = model.predict(data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf686f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b958c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760d0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d4570f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592cc21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5424e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df670c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112db055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf088ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf509a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d979ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
